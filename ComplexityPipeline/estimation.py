# -*- coding: utf-8 -*-
"""Estimation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iwhkMJhCG42ZPr83hH6PQvn7jMI3Dtne
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import re
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple
import warnings
import joblib
import os
warnings.filterwarnings('ignore')

# VFX Template Analysis Classes (from your original code)
@dataclass
class VFXRequirements:
    """Structured representation of VFX requirements from template."""
    lens_model: Optional[str] = None
    focal_length: Optional[str] = None
    camera_model: Optional[str] = None
    filmback: Optional[str] = None
    brief_description: Optional[str] = None
    lens_grid: bool = False
    assets_3d_models: bool = False
    assets_ref_images: bool = False
    assets_lidar: bool = False
    measurements: Optional[str] = None
    set_location: Optional[str] = None
    priority_1_completeness: float = 0.0
    priority_2_completeness: float = 0.0
    priority_3_completeness: float = 0.0
    estimated_additional_hours: float = 0.0
    completeness_score: float = 0.0

class VFXTemplateAnalyzer:
    def __init__(self):
        """Initialize the VFX Template Analyzer."""
        print("üîß Initializing VFX Template Analyzer...")

    def parse_template_structure(self, description: str) -> VFXRequirements:
        """Parse VFX description according to the template structure."""
        requirements = VFXRequirements()

        if pd.isna(description) or not str(description).strip():
            return requirements

        text = str(description).lower().strip()

        # === PRIORITY 1 PARSING ===
        # Lens/Focal Length
        lens_patterns = [
            r'lens[:\s]*([0-9]+(?:\.[0-9]+)?mm)',
            r'focal[:\s]*([0-9]+(?:\.[0-9]+)?mm)',
            r'([0-9]+(?:\.[0-9]+)?mm)',
            r'lens[:\s]*([a-zA-Z0-9\s]+?)(?:\n|camera|filmback|brief)',
        ]
        for pattern in lens_patterns:
            match = re.search(pattern, text)
            if match:
                lens_info = match.group(1).strip()
                if 'mm' in lens_info:
                    requirements.focal_length = lens_info
                else:
                    requirements.lens_model = lens_info
                break

        # Camera Model
        camera_patterns = [
            r'camera[:\s]*([a-zA-Z0-9\s]+?)(?:\n|filmback|brief)',
            r'(arri alexa[a-zA-Z0-9\s]*)',
            r'(red [a-zA-Z0-9\s]*)',
            r'(canon [a-zA-Z0-9\s]*)',
            r'(sony [a-zA-Z0-9\s]*)',
            r'(samsung galaxy s[0-9]+)',
            r'(iphone [0-9]+[a-zA-Z\s]*)',
        ]
        for pattern in camera_patterns:
            match = re.search(pattern, text)
            if match:
                requirements.camera_model = match.group(1).strip()
                break

        # Filmback/Sensor
        filmback_patterns = [
            r'filmback[:\s]*([a-zA-Z0-9\s\.]+?)(?:\n|brief)',
            r'sensor[:\s]*([a-zA-Z0-9\s\.]+?)(?:\n|brief)',
            r'(full frame|super 35|aps-c|micro four thirds)',
        ]
        for pattern in filmback_patterns:
            match = re.search(pattern, text)
            if match:
                requirements.filmback = match.group(1).strip()
                break

        # Brief Description
        brief_patterns = [
            r'brief[:\s]*(.*?)(?:priority|lens grid|assets|measurements|set location|$)',
            r'vfx description[:\s]*(.*?)(?:priority|lens grid|assets|measurements|set location|$)',
            r'description[:\s]*(.*?)(?:priority|lens grid|assets|measurements|set location|$)',
        ]
        for pattern in brief_patterns:
            match = re.search(pattern, text, re.DOTALL)
            if match:
                requirements.brief_description = match.group(1).strip()[:500]
                break

        if not requirements.brief_description and len(description) > 0:
            requirements.brief_description = description[:200]

        # === PRIORITY 2 PARSING ===
        requirements.lens_grid = any(term in text for term in [
            'lens grid', 'lensgrid', 'grid', 'calibration grid'
        ])

        requirements.assets_3d_models = any(term in text for term in [
            '3d model', '3d asset', 'geometry', 'mesh', 'obj', 'fbx', 'alembic'
        ])

        requirements.assets_ref_images = any(term in text for term in [
            'reference image', 'ref image', 'reference photo', 'ref photo', 'hdri'
        ])

        requirements.assets_lidar = any(term in text for term in [
            'lidar', 'laser scan', 'point cloud', 'scan data'
        ])

        # === PRIORITY 3 PARSING ===
        measurement_patterns = [
            r'measurement[s]?[:\s]*(.*?)(?:set location|$)',
            r'dimension[s]?[:\s]*(.*?)(?:set location|$)',
            r'([0-9]+(?:\.[0-9]+)?\s*(?:m|cm|mm|ft|in))',
        ]
        for pattern in measurement_patterns:
            match = re.search(pattern, text, re.DOTALL)
            if match:
                requirements.measurements = match.group(1).strip()[:200]
                break

        location_patterns = [
            r'set location[:\s]*(.*?)$',
            r'location[:\s]*(.*?)$',
            r'filmed at[:\s]*(.*?)$',
            r'shot at[:\s]*(.*?)$',
        ]
        for pattern in location_patterns:
            match = re.search(pattern, text, re.DOTALL)
            if match:
                requirements.set_location = match.group(1).strip()[:200]
                break

        # Calculate completeness scores
        requirements = self._calculate_completeness(requirements)
        return requirements

    def _calculate_completeness(self, req: VFXRequirements) -> VFXRequirements:
        """Calculate completeness scores and time impact."""
        # Priority 1: Critical technical info
        p1_fields = [
            req.lens_model or req.focal_length,
            req.camera_model,
            req.filmback,
            req.brief_description
        ]
        req.priority_1_completeness = sum(1 for field in p1_fields if field) / len(p1_fields)

        # Priority 2: Asset requirements
        p2_fields = [
            req.lens_grid,
            req.assets_3d_models or req.assets_ref_images or req.assets_lidar
        ]
        req.priority_2_completeness = sum(1 for field in p2_fields if field) / len(p2_fields)

        # Priority 3: Additional context
        p3_fields = [req.measurements, req.set_location]
        req.priority_3_completeness = sum(1 for field in p3_fields if field) / len(p3_fields)

        # Overall completeness (weighted)
        total_weight = 0.6 + 0.3 + 0.1
        weighted_sum = (req.priority_1_completeness * 0.6) + \
                      (req.priority_2_completeness * 0.3) + \
                      (req.priority_3_completeness * 0.1)
        req.completeness_score = weighted_sum / total_weight

        # Estimate additional hours needed
        missing_p1 = (1 - req.priority_1_completeness) * 4
        missing_p2 = (1 - req.priority_2_completeness) * 1
        req.estimated_additional_hours = missing_p1 + missing_p2

        return req

class VFXBidPredictor:
    """Complete VFX Bid Hours Prediction Pipeline"""

    def __init__(self):
        """Initialize the predictor."""
        self.analyzer = VFXTemplateAnalyzer()
        self.models = {
            'Linear Regression': LinearRegression(),
            'Ridge Regression': Ridge(alpha=1.0),
            'Lasso Regression': Lasso(alpha=1.0),
            'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
            'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),
            'Support Vector Regression': SVR(kernel='rbf', C=1.0, gamma='scale')
        }
        self.best_model = None
        self.best_model_name = None
        self.feature_scaler = StandardScaler()
        self.tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
        self.label_encoders = {}
        self.feature_names = []
        self.results_df = None
        self.trained_categorical_values = {} # Store unique values from training for categorical features

    def load_and_prepare_data(self, csv_file_path, description_column='VFX_description',
                            target_column='bid_hours', additional_features=None):
        """
        Load CSV data and prepare it for training.

        Args:
            csv_file_path: Path to the CSV file
            description_column: Name of the column containing VFX descriptions
            target_column: Name of the column containing bid hours
            additional_features: List of additional feature columns to include
        """
        print("üìÇ Loading and preparing data...")

        # Load CSV
        try:
            self.df = pd.read_csv(csv_file_path)
            print(f"‚úÖ Loaded {len(self.df)} records from {csv_file_path}")
        except FileNotFoundError:
            print(f"‚ùå Error: File {csv_file_path} not found.")
            return None
        except Exception as e:
            print(f"‚ùå Error loading CSV: {str(e)}")
            return None

        # Check required columns
        if description_column not in self.df.columns:
            print(f"‚ùå Error: Description column '{description_column}' not found.")
            print(f"Available columns: {list(self.df.columns)}")
            return None

        if target_column not in self.df.columns:
            print(f"‚ùå Error: Target column '{target_column}' not found.")
            print(f"Available columns: {list(self.df.columns)}")
            return None

        # Clean data
        print("üßπ Cleaning data...")
        self.df = self.df.dropna(subset=[target_column])  # Remove rows with missing targets
        self.df[description_column] = self.df[description_column].fillna('')  # Fill missing descriptions

        # Remove rows with zero or negative bid hours
        initial_count = len(self.df)
        self.df = self.df[self.df[target_column] > 0]
        removed_count = initial_count - len(self.df)
        if removed_count > 0:
            print(f"üóëÔ∏è Removed {removed_count} rows with non-positive bid hours")

        print(f"‚úÖ Clean dataset: {len(self.df)} records")
        print(f"üìä Bid hours range: {self.df[target_column].min():.1f} - {self.df[target_column].max():.1f}")

        return self.df

    def extract_features(self, description_column='VFX_description', additional_features=None):
        """Extract features from VFX descriptions and additional columns."""
        print("üîß Extracting features...")

        # 1. TF-IDF Features from descriptions
        print("  üìù Extracting TF-IDF features...")
        descriptions = self.df[description_column].fillna('').astype(str)
        tfidf_features = self.tfidf_vectorizer.fit_transform(descriptions).toarray()
        tfidf_feature_names = [f"tfidf_{name}" for name in self.tfidf_vectorizer.get_feature_names_out()]
        print(f"  ‚úÖ Extracted {tfidf_features.shape[1]} TF-IDF features")

        # 2. VFX Template Analysis Features
        print("  üé¨ Running VFX template analysis...")
        template_features = []
        template_feature_names = [
            'priority_1_completeness', 'priority_2_completeness', 'priority_3_completeness',
            'overall_completeness', 'estimated_additional_hours', 'has_lens_grid',
            'has_3d_models', 'has_ref_images', 'has_lidar'
        ]

        for idx, description in enumerate(descriptions):
            req = self.analyzer.parse_template_structure(description)
            template_features.append([
                req.priority_1_completeness,
                req.priority_2_completeness,
                req.priority_3_completeness,
                req.completeness_score,
                req.estimated_additional_hours,
                int(req.lens_grid),
                int(req.assets_3d_models),
                int(req.assets_ref_images),
                int(req.assets_lidar)
            ])

        template_features = np.array(template_features)
        print(f"  ‚úÖ Extracted {template_features.shape[1]} template analysis features")

        # 3. Additional Features (if specified)
        additional_feature_arrays = []
        additional_feature_names = []

        if additional_features:
            print(f"  üìã Processing additional features: {additional_features}")
            for feature in additional_features:
                if feature in self.df.columns:
                    feature_data = self.df[feature].values

                    # Handle categorical features
                    if self.df[feature].dtype == 'object':
                        if feature not in self.label_encoders:
                            self.label_encoders[feature] = LabelEncoder()
                            # Fit on training data, handle potential NaNs
                            values_to_fit = self.df[feature].fillna('Unknown')
                            self.label_encoders[feature].fit(values_to_fit)
                            self.trained_categorical_values[feature] = set(self.label_encoders[feature].classes_) # Store for prediction
                            encoded_data = self.label_encoders[feature].transform(values_to_fit)
                        else:
                             # Should not happen during training feature extraction
                             encoded_data = self.label_encoders[feature].transform(
                                self.df[feature].fillna('Unknown')
                            )

                        additional_feature_arrays.append(encoded_data.reshape(-1, 1))
                        additional_feature_names.append(f"{feature}_encoded")
                    else:
                        # Numerical features
                        numerical_series = pd.Series(feature_data)
                        numerical_data = pd.to_numeric(numerical_series, errors='coerce').fillna(0).values
                        additional_feature_arrays.append(numerical_data.reshape(-1, 1))
                        additional_feature_names.append(feature)
                else:
                    print(f"  ‚ö†Ô∏è Warning: Feature '{feature}' not found in dataset")

        # Combine all features
        feature_arrays = [tfidf_features, template_features]
        self.feature_names = tfidf_feature_names + template_feature_names

        if additional_feature_arrays:
            feature_arrays.extend(additional_feature_arrays)
            self.feature_names.extend(additional_feature_names)
            print(f"  ‚úÖ Added {len(additional_feature_names)} additional features")

        # Stack all features
        self.X = np.hstack(feature_arrays)
        print(f"üéØ Total features extracted: {self.X.shape[1]}")

        return self.X

    def train_models(self, target_column='bid_hours', test_size=0.2, random_state=42):
        """Train multiple models and select the best one."""
        print("\nüöÄ Training models...")

        # Prepare target
        self.y = self.df[target_column].values

        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            self.X, self.y, test_size=test_size, random_state=random_state
        )

        # Scale features
        X_train_scaled = self.feature_scaler.fit_transform(X_train)
        X_test_scaled = self.feature_scaler.transform(X_test)

        print(f"üìä Training set: {X_train.shape[0]} samples")
        print(f"üìä Test set: {X_test.shape[0]} samples")
        print("-" * 60)

        results = []
        best_score = -np.inf

        for name, model in self.models.items():
            print(f"Training {name}...")

            try:
                # Train model
                model.fit(X_train_scaled, y_train)

                # Make predictions
                y_pred = model.predict(X_test_scaled)

                # Calculate metrics
                mse = mean_squared_error(y_test, y_pred)
                rmse = np.sqrt(mse)
                mae = mean_absolute_error(y_test, y_pred)
                r2 = r2_score(y_test, y_pred)

                # Cross-validation
                cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='r2')
                cv_mean = cv_scores.mean()
                cv_std = cv_scores.std()

                # Store results
                result = {
                    'Model': name,
                    'RMSE': rmse,
                    'MAE': mae,
                    'R¬≤': r2,
                    'CV_R¬≤_Mean': cv_mean,
                    'CV_R¬≤_Std': cv_std
                }
                results.append(result)

                # Track best model
                if r2 > best_score:
                    best_score = r2
                    self.best_model = model
                    self.best_model_name = name
                    self.test_predictions = y_pred
                    self.y_test = y_test

                print(f"  ‚úÖ RMSE: {rmse:.2f}, MAE: {mae:.2f}, R¬≤: {r2:.3f}, CV R¬≤: {cv_mean:.3f}¬±{cv_std:.3f}")

            except Exception as e:
                print(f"  ‚ùå Error training {name}: {str(e)}")
                continue

        # Create results DataFrame
        self.results_df = pd.DataFrame(results).sort_values('R¬≤', ascending=False)

        print("\n" + "="*60)
        print("üèÜ MODEL PERFORMANCE SUMMARY")
        print("="*60)
        print(self.results_df.to_string(index=False, float_format='%.3f'))
        print(f"\nü•á Best Model: {self.best_model_name} (R¬≤ = {best_score:.3f})")

        return self.results_df

    def analyze_predictions(self):
        """Create detailed analysis of predictions."""
        if self.best_model is None:
            print("‚ùå No trained model available for analysis.")
            return

        print("\nüìà PREDICTION ANALYSIS")
        print("="*50)

        # Calculate detailed metrics
        rmse = np.sqrt(mean_squared_error(self.y_test, self.test_predictions))
        mae = mean_absolute_error(self.y_test, self.test_predictions)
        r2 = r2_score(self.y_test, self.test_predictions)
        # Handle potential division by zero or very small actual values for MAPE
        mape = np.mean(np.abs((self.y_test - self.test_predictions) / np.maximum(self.y_test, 1e-6))) * 100


        print(f"Root Mean Square Error (RMSE): {rmse:.2f} hours")
        print(f"Mean Absolute Error (MAE): {mae:.2f} hours")
        print(f"R¬≤ Score: {r2:.3f}")
        print(f"Mean Absolute Percentage Error: {mape:.1f}%")

        # Create visualizations
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))

        # 1. Actual vs Predicted
        axes[0, 0].scatter(self.y_test, self.test_predictions, alpha=0.6, color='blue')
        axes[0, 0].plot([self.y_test.min(), self.y_test.max()],
                       [self.y_test.min(), self.y_test.max()], 'r--', lw=2)
        axes[0, 0].set_xlabel('Actual Bid Hours')
        axes[0, 0].set_ylabel('Predicted Bid Hours')
        axes[0, 0].set_title(f'Actual vs Predicted - {self.best_model_name}')
        axes[0, 0].grid(True, alpha=0.3)

        # 2. Residuals
        residuals = self.y_test - self.test_predictions
        axes[0, 1].scatter(self.test_predictions, residuals, alpha=0.6, color='green')
        axes[0, 1].axhline(y=0, color='r', linestyle='--')
        axes[0, 1].set_xlabel('Predicted Bid Hours')
        axes[0, 1].set_ylabel('Residuals')
        axes[0, 1].set_title('Residuals Plot')
        axes[0, 1].grid(True, alpha=0.3)

        # 3. Residuals distribution
        axes[1, 0].hist(residuals, bins=30, alpha=0.7, color='orange', edgecolor='black')
        axes[1, 0].set_xlabel('Residuals')
        axes[1, 0].set_ylabel('Frequency')
        axes[1, 0].set_title('Distribution of Residuals')
        axes[1, 0].grid(True, alpha=0.3)

        # 4. Feature importance (if available)
        if hasattr(self.best_model, 'feature_importances_'):
            importances = self.best_model.feature_importances_
            indices = np.argsort(importances)[-15:]  # Top 15 features

            axes[1, 1].barh(range(len(indices)), importances[indices])
            axes[1, 1].set_yticks(range(len(indices)))
            if len(self.feature_names) > 0:
                axes[1, 1].set_yticklabels([self.feature_names[i] for i in indices])
            axes[1, 1].set_xlabel('Feature Importance')
            axes[1, 1].set_title('Top 15 Feature Importances')
        else:
            axes[1, 1].text(0.5, 0.5, 'Feature importance not available\nfor this model type',
                           ha='center', va='center', transform=axes[1, 1].transAxes)
            axes[1, 1].set_title('Feature Importance')

        plt.tight_layout()
        plt.show()

        # Error analysis by ranges
        print("\nüìä Error Analysis by Bid Hour Ranges:")
        print("-" * 40)
        ranges = [(0, 10), (10, 50), (50, 100), (100, float('inf'))]
        range_names = ['0-10h', '10-50h', '50-100h', '100h+']

        for i, (low, high) in enumerate(ranges):
            mask = (self.y_test >= low) & (self.y_test < high)
            if mask.sum() > 0:
                range_mae = mean_absolute_error(self.y_test[mask], self.test_predictions[mask])
                range_count = mask.sum()
                print(f"{range_names[i]:>8}: {range_count:>3} samples, MAE: {range_mae:.2f} hours")

    def predict_new_descriptions(self, new_descriptions, additional_feature_values=None):
        """
        Predict bid hours for new VFX descriptions.
        Modified version that works with pre-trained models without needing self.X
        """
        if self.best_model is None:
            print("‚ùå No trained model available. Please train or load the model first.")
            return None

        print("üîÆ Making predictions for new descriptions...")

        # Extract TF-IDF features
        tfidf_features = self.tfidf_vectorizer.transform(new_descriptions).toarray()

        # Extract template analysis features
        template_features = []
        for description in new_descriptions:
            req = self.analyzer.parse_template_structure(description)
            template_features.append([
                req.priority_1_completeness,
                req.priority_2_completeness,
                req.priority_3_completeness,
                req.completeness_score,
                req.estimated_additional_hours,
                int(req.lens_grid),
                int(req.assets_3d_models),
                int(req.assets_ref_images),
                int(req.assets_lidar)
            ])

        template_features = np.array(template_features)

        # Combine features
        feature_arrays = [tfidf_features, template_features]

        # Add additional features if provided
        if additional_feature_values:
            for feature_name, values in additional_feature_values.items():
                if feature_name in self.label_encoders:
                    # Categorical feature
                    le = self.label_encoders[feature_name]
                    processed_values = []
                    for val in values:
                        if val not in le.classes_:
                            replacement = 'Unknown' if 'Unknown' in le.classes_ else le.classes_[0]
                            print(f"‚ö†Ô∏è Replacing unseen category '{val}' with '{replacement}'")
                            processed_values.append(replacement)
                        else:
                            processed_values.append(val)
                    encoded = le.transform(processed_values).reshape(-1, 1)
                    feature_arrays.append(encoded)
                elif feature_name in self.feature_names:
                    # Numerical feature
                    numerical_values = np.array(values).reshape(-1, 1)
                    feature_arrays.append(numerical_values)
                else:
                    print(f"‚ö†Ô∏è Feature '{feature_name}' not found in trained features")

        # Stack all features
        try:
            X_new = np.hstack(feature_arrays)
        except ValueError as e:
            print(f"‚ùå Feature stacking failed: {e}")
            print("Please ensure all additional features match the training data format")
            return None

        # Scale features and predict
        X_new_scaled = self.feature_scaler.transform(X_new)
        predictions = self.best_model.predict(X_new_scaled)
        return np.maximum(predictions, 0)  # Ensure non-negative
        def save_model(self, filepath='vfx_bid_predictor.joblib'):
            """Save the trained model and all components."""
            model_data = {
                'best_model': self.best_model,
                'best_model_name': self.best_model_name,
                'feature_scaler': self.feature_scaler,
                'tfidf_vectorizer': self.tfidf_vectorizer,
                'label_encoders': self.label_encoders,
                'feature_names': self.feature_names,
                'analyzer': self.analyzer,
                'results_df': self.results_df,
                'trained_categorical_values': self.trained_categorical_values # Save this as well
            }
            joblib.dump(model_data, filepath)
            print(f"‚úÖ Model saved to {filepath}")

    def load_model(self, filepath):
        """Load a previously trained model."""
        model_data = joblib.load(filepath)
        self.best_model = model_data['best_model']
        self.best_model_name = model_data['best_model_name']
        self.feature_scaler = model_data['feature_scaler']
        self.tfidf_vectorizer = model_data['tfidf_vectorizer']
        self.label_encoders = model_data['label_encoders']
        self.feature_names = model_data['feature_names']
        self.analyzer = model_data['analyzer']
        self.results_df = model_data['results_df']
        self.trained_categorical_values = model_data.get('trained_categorical_values', {}) # Load with default for backward compatibility
        print(f"‚úÖ Model loaded from {filepath}")


# Function to load CSV and run complete pipeline
def load_csv_and_train_model(csv_file_path, description_column='VFX_description',
                            target_column='bid_hours', remove_outliers=True):
    """
    Load CSV file and run the complete VFX bid prediction pipeline.

    Args:
        csv_file_path: Path to your CSV file
        description_column: Name of column with VFX descriptions (default: 'VFX_description')
        target_column: Name of column with bid hours (default: 'bid_hours')
        remove_outliers: Whether to remove outliers in bid_hours (default: True)

    Returns:
        Trained VFXBidPredictor object
    """
    print("üé¨ LOADING CSV AND TRAINING VFX BID PREDICTION MODEL")
    print("="*70)

    # Load CSV file
    try:
        print(f"üìÇ Loading data from: {csv_file_path}")
        df = pd.read_csv(csv_file_path)
        print(f"‚úÖ Successfully loaded {len(df)} rows and {len(df.columns)} columns")
        print(f"üìã Columns: {list(df.columns)}")

    except FileNotFoundError:
        print(f"‚ùå Error: File '{csv_file_path}' not found.")
        print("Please check the file path and make sure the file exists.")
        return None
    except Exception as e:
        print(f"‚ùå Error loading CSV: {str(e)}")
        return None

    # Data cleaning and preparation
    print(f"\nüßπ Cleaning and preparing data...")

    # Check for required columns
    if description_column not in df.columns:
        print(f"‚ùå Error: Description column '{description_column}' not found.")
        print(f"Available columns: {list(df.columns)}")
        return None

    if target_column not in df.columns:
        print(f"‚ùå Error: Target column '{target_column}' not found.")
        print(f"Available columns: {list(df.columns)}")
        return None

    # Initial data info
    print(f"üìä Initial dataset shape: {df.shape}")
    print(f"üìù Missing values in {description_column}: {df[description_column].isna().sum()}")
    print(f"üéØ Missing values in {target_column}: {df[target_column].isna().sum()}")

    # Remove rows with missing critical data
    initial_rows = len(df)
    df = df.dropna(subset=[description_column, target_column])
    removed_missing = initial_rows - len(df)
    if removed_missing > 0:
        print(f"üóëÔ∏è Removed {removed_missing} rows with missing critical data")

    # Remove rows with non-positive bid hours
    df = df[df[target_column] > 0]
    removed_non_positive = len(df) - (initial_rows - removed_missing)
    if removed_non_positive < 0:
        removed_non_positive = (initial_rows - removed_missing) - len(df)
        if removed_non_positive > 0:
            print(f"üóëÔ∏è Removed {removed_non_positive} rows with non-positive bid hours")

    # Outlier removal
    if remove_outliers:
        print(f"üîç Identifying and removing outliers in '{target_column}'...")
        Q1 = df[target_column].quantile(0.25)
        Q3 = df[target_column].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        outliers = df[(df[target_column] < lower_bound) | (df[target_column] > upper_bound)]
        if len(outliers) > 0:
            print(f"‚ö†Ô∏è Found {len(outliers)} outliers in '{target_column}' (< {lower_bound:.1f} or > {upper_bound:.1f})")
            df = df[(df[target_column] >= lower_bound) & (df[target_column] <= upper_bound)]
            print(f"üóëÔ∏è Removed {len(outliers)} outlier rows")

    print(f"‚úÖ Final clean dataset: {len(df)} rows")
    print(f"üìä {target_column} range: {df[target_column].min():.1f} - {df[target_column].max():.1f} hours")

    # Run the pipeline with cleaned data
    return run_vfx_pipeline_for_your_data(df)

# Specialized function for your specific dataset
def run_vfx_pipeline_for_your_data(df_cleaned):
    """
    Run the VFX bid prediction pipeline specifically for your cleaned dataset.

    Args:
        df_cleaned: Your cleaned pandas DataFrame
    """
    print("üé¨ VFX BID HOURS PREDICTION PIPELINE - YOUR DATA")
    print("="*60)
    print(f"üìä Dataset shape: {df_cleaned.shape}")
    print(f"üìã Columns: {list(df_cleaned.columns)}")

    # Initialize predictor
    predictor = VFXBidPredictor()
    predictor.df = df_cleaned.copy()

    # Define features to use
    description_column = 'VFX_description'
    target_column = 'bid_hours'
    additional_features = ['complexity_task', 'task_name', 'project_name', 'notes_count']

    print(f"\nüéØ Target: {target_column}")
    print(f"üìù Description column: {description_column}")
    print(f"‚ûï Additional features: {additional_features}")

    # Verify data quality
    print(f"\nüîç Data Quality Check:")
    print(f"  ‚Ä¢ Total records: {len(predictor.df)}")
    print(f"  ‚Ä¢ Missing descriptions: {predictor.df[description_column].isna().sum()}")
    print(f"  ‚Ä¢ Missing targets: {predictor.df[target_column].isna().sum()}")
    print(f"  ‚Ä¢ Bid hours range: {predictor.df[target_column].min():.1f} - {predictor.df[target_column].max():.1f}")

    # Show distribution of key categorical features
    print(f"\nüìà Feature Distributions:")
    for feature in ['complexity_task', 'task_name']:
        if feature in predictor.df.columns:
            print(f"\n{feature}:")
            counts = predictor.df[feature].value_counts().head(5)
            for value, count in counts.items():
                print(f"  ‚Ä¢ {value}: {count}")

    # Extract features
    print(f"\nüîß Extracting features...")
    X = predictor.extract_features(description_column, additional_features)
    if X is None:
        return None

    # Show feature summary
    print(f"\nüìä Feature Summary:")
    print(f"  ‚Ä¢ TF-IDF features from descriptions: ~1000")
    print(f"  ‚Ä¢ Template analysis features: 9")
    print(f"  ‚Ä¢ Additional features: {len([f for f in additional_features if f in predictor.df.columns])}")
    print(f"  ‚Ä¢ Total features: {X.shape[1]}")

    # Train models
    print(f"\nüöÄ Training models...")
    results = predictor.train_models(target_column)

    # Analyze predictions
    predictor.analyze_predictions()

    # Feature importance analysis for your data
    if hasattr(predictor.best_model, 'feature_importances_'):
        print(f"\nüîç Top Features Affecting Bid Hours:")
        importances = predictor.best_model.feature_importances_
        feature_importance_df = pd.DataFrame({
            'feature': predictor.feature_names,
            'importance': importances
        }).sort_values('importance', ascending=False)

        print("\nTop 20 Most Important Features:")
        for idx, row in feature_importance_df.head(20).iterrows():
            print(f"  {row['importance']:.4f} - {row['feature']}")

    # Analysis by complexity and task type
    print(f"\nüìä Prediction Analysis by Categories:")

    # Group predictions by complexity
    complexity_analysis = predictor.df.groupby('complexity_task').agg({
        'bid_hours': ['count', 'mean', 'std']
    }).round(2)
    complexity_analysis.columns = ['Count', 'Avg_Hours', 'Std_Hours']
    print(f"\nBy Complexity:")
    print(complexity_analysis)

    # Group by task type
    task_analysis = predictor.df.groupby('task_name').agg({
        'bid_hours': ['count', 'mean', 'std']
    }).round(2)
    task_analysis.columns = ['Count', 'Avg_Hours', 'Std_Hours']
    print(f"\nBy Task Type (Top 10):")
    print(task_analysis.head(10))

    # Save model
    predictor.save_model('vfx_bid_predictor_your_data.joblib')

    print(f"\nüéØ Pipeline complete!")
    print(f"ü•á Best model: {predictor.best_model_name}")

    # Example predictions with your data structure
    print(f"\nüìù Example: Making predictions for new VFX tasks")

    # Get actual values from your dataset to avoid label encoder errors
    available_complexities = list(predictor.df['complexity_task'].unique())
    available_tasks = list(predictor.df['task_name'].unique())
    available_projects = list(predictor.df['project_name'].unique())

    print(f"üìä Available values in your data:")
    print(f"  ‚Ä¢ Complexity levels: {available_complexities}")
    print(f"  ‚Ä¢ Task types: {available_tasks[:5]}{'...' if len(available_tasks) > 5 else ''}")
    print(f"  ‚Ä¢ Projects: {available_projects[:3]}{'...' if len(available_projects) > 3 else ''}")

    # Create sample predictions using actual values from your data
    sample_data = [
        {
            'description': "Camera: ARRI Alexa, Lens: 50mm, Brief: Complex green screen compositing with 3D environment integration, multiple layers",
            'complexity_task': available_complexities[0] if available_complexities else 'Medium',
            'task_name': available_tasks[0] if available_tasks else 'tracking',
            'project_name': available_projects[0] if available_projects else 'test_project',
            'notes_count': 3
        },
        {
            'description': "Basic tracking shot with simple object removal",
            'complexity_task': available_complexities[-1] if len(available_complexities) > 1 else available_complexities[0],
            'task_name': available_tasks[1] if len(available_tasks) > 1 else available_tasks[0],
            'project_name': available_projects[0] if available_projects else 'test_project',
            'notes_count': 1
        },
        { # Add a sample with a new, unseen task name
            'description': "Detailed roto-scoping for character animation",
            'complexity_task': available_complexities[0] if available_complexities else 'Medium',
            'task_name': 'Rotoscoping', # This will be unseen
            'project_name': available_projects[0] if available_projects else 'test_project',
            'notes_count': 2
        }
    ]

    try:
        sample_descriptions = [item['description'] for item in sample_data]
        additional_values = {
            'complexity_task': [item['complexity_task'] for item in sample_data],
            'task_name': [item['task_name'] for item in sample_data],
            'project_name': [item['project_name'] for item in sample_data],
            'notes_count': [item['notes_count'] for item in sample_data]
        }

        sample_predictions = predictor.predict_new_descriptions(
            sample_descriptions, additional_values
        )

        print(f"\nüéØ Prediction Results:")
        if sample_predictions is not None:
            for i, (data, pred) in enumerate(zip(sample_data, sample_predictions)):
                print(f"\nüìã Sample {i+1}:")
                print(f"  Task: {data['task_name']} ({data['complexity_task']})")
                print(f"  Description: {data['description'][:80]}...")
                print(f"  üéØ Predicted Hours: {pred:.1f}")
        else:
            print("Prediction failed.")


    except Exception as e:
        print(f"‚ùå Error in sample predictions: {str(e)}")
        # print("üí° This might be due to label encoding issues with categorical features.")
        # print("   When making real predictions, ensure you use values that exist in the training data.")

    return predictor

# Quick start functions for different scenarios
def quick_train_from_csv(csv_file_path):
    """
    Quick function to train model from CSV with default settings.
    Assumes your CSV has 'VFX_description' and 'bid_hours' columns.

    Args:
        csv_file_path: Path to your CSV file

    Returns:
        Trained predictor
    """
    return load_csv_and_train_model(csv_file_path)

def train_with_custom_columns(csv_file_path, desc_col, target_col):
    """
    Train model with custom column names.

    Args:
        csv_file_path: Path to your CSV file
        desc_col: Name of description column
        target_col: Name of target column

    Returns:
        Trained predictor
    """
    return load_csv_and_train_model(csv_file_path, desc_col, target_col)

